tmpÎ¸0 = learningRate * 1/m * Î£ (estimatePrice(mileage[i]) âˆ’ price[i])
tmpÎ¸1 = learningRate * 1/m * Î£ (estimatePrice(mileage[i]) âˆ’ price[i]) * mileage[i]

ğŸ‘‰ m reprÃ©sente le nombre total dâ€™exemples (lignes) dans un dataset, câ€™est-Ã -dire le nombre de voitures dans le fichier data.csv.

Câ€™est utilisÃ© pour faire la moyenne des erreurs dans lâ€™algorithme de gradient descent.

Le learning rate (ou taux dâ€™apprentissage) dÃ©termine Ã  quelle vitesse les paramÃ¨tres sont ajustÃ©s.

Mais ici, on travaille avec des kilomÃ©trages (ex: 200000), donc :
Les valeurs dâ€™entrÃ©e sont grandes

Les gradients peuvent Ãªtre trÃ¨s grands â†’ le risque est que lâ€™algorithme diverge si le learning rate est trop Ã©levÃ©

ğŸ‘‰ On met un petit learning_rate pour Ã©viter des explosions numÃ©riques.

normaliser les donnÃ©es (ex: diviser les km par 100000) pour pouvoir augmenter le learning_rate (ex: Ã  0.01).

Les iterations :
nombre de boucles de mise Ã  jour de theta0 et theta1.

Pourquoi autant ?

Parce que le learning rate est petit â†’ les mises Ã  jour sont lentes

Il faut donc plus dâ€™itÃ©rations pour atteindre une solution correcte


Mesures de prÃ©cision possibles :
MSE	Moyenne des erreurs au carrÃ©	Comprendre lâ€™erreur globale
RMSE	Racine de MSE (en â‚¬ ici)	InterprÃ©tation en unitÃ©s rÃ©elles
MAE	Moyenne des erreurs absolues	Moins sensible aux outliers
RÂ² (coefficient de dÃ©termination)	Pourcentage de variance expliquÃ©e	Score de performance global